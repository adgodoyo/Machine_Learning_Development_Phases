{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fill-Mask (Completado de texto con máscara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para 'fill-mask':\n",
      "Palabra predicha: necesario | Score: 0.1978\n",
      "Palabra predicha: bueno | Score: 0.1463\n",
      "Palabra predicha: esencial | Score: 0.1131\n",
      "Palabra predicha: importante | Score: 0.0878\n",
      "Palabra predicha: vital | Score: 0.0630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fill_mask_pipe = pipeline(\"fill-mask\", model=\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "frase_fill_mask = \"El cambio climático es [MASK] para el futuro.\"\n",
    "resultados_fill_mask = fill_mask_pipe(frase_fill_mask)\n",
    "\n",
    "print(\"Resultados para 'fill-mask':\")\n",
    "for res in resultados_fill_mask:\n",
    "    print(f\"Palabra predicha: {res['token_str']} | Score: {res['score']:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Classification (Clasificación de texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza para clasificar el texto. En este ejemplo, se clasifica el sentimiento de un texto como positivo, negativo o neutral.\n",
    "1 estrella: Muy negativo.\n",
    "2 estrellas: Negativo.\n",
    "3 estrellas: Neutro.\n",
    "4 estrellas: Positivo.\n",
    "5 estrellas: Muy positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para 'text-classification':\n",
      "[{'label': '5 stars', 'score': 0.8085700869560242}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_class_pipe = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "texto_clasificacion = \"Este producto es excelente y de alta calidad.\"\n",
    "resultado_clasificacion = text_class_pipe(texto_clasificacion)\n",
    "\n",
    "print(\"Resultados para 'text-classification':\")\n",
    "print(resultado_clasificacion)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Token Classification (Clasificación por token / NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realiza reconocimiento de entidades nombradas (NER), etiquetando palabras como organizaciones, personas, lugares, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para 'token-classification' (NER):\n",
      "Entidad: Apple | Tipo: B-ORG | Score: 0.9971\n",
      "Entidad: Bogotá | Tipo: B-LOC | Score: 0.9998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_class_pipe = pipeline(\"token-classification\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\")\n",
    "frase_token_class = \"Apple está abriendo una nueva oficina en Bogotá.\"\n",
    "resultados_token_class = token_class_pipe(frase_token_class)\n",
    "\n",
    "print(\"Resultados para 'token-classification' (NER):\")\n",
    "for res in resultados_token_class:\n",
    "    print(f\"Entidad: {res['word']} | Tipo: {res['entity']} | Score: {res['score']:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Generation (Generación de texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para 'text-generation':\n",
      "La inteligencia artificial es una tecnología no invasiva que combina el control de campo, la detección de objetos y habilidades de sensores, y la capacidad de identificar y detectar objetos muy pequeños. Además, al control del campo se le hace un sistema de comunicación de\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_gen_pipe = pipeline(\"text-generation\", model=\"datificate/gpt2-small-spanish\")\n",
    "frase_inicial_text_gen = \"La inteligencia artificial es\"\n",
    "resultado_text_gen = text_gen_pipe(frase_inicial_text_gen, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"Resultados para 'text-generation':\")\n",
    "for res in resultado_text_gen:\n",
    "    print(res['generated_text'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summarization (Resumen de texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_pipe = pipeline(\"summarization\", model=\"mrm8488/t5-base-finetuned-summarize-news\")\n",
    "texto_largo_summarization = \"\"\"\n",
    "La inteligencia artificial está transformando muchas industrias. Empresas como Google, Microsoft, y Amazon están invirtiendo en \n",
    "nuevas tecnologías para mejorar la eficiencia y la toma de decisiones automatizada. Estas innovaciones podrían cambiar la forma \n",
    "en que trabajamos y vivimos.\n",
    "\"\"\"\n",
    "resumen = summarization_pipe(texto_largo_summarization)\n",
    "\n",
    "print(\"Resultados para 'summarization':\")\n",
    "print(resumen[0]['summary_text'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Translation (Traducción de texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ddfff36e1e472a89291aef415babf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773e42d6ced24cbd856f349699f069f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbef08cce34347bdab7aa950829c1060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1ba25b9b4b498d8797a90e8453cde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m translation_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation_en_to_es\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHelsinki-NLP/opus-mt-en-es\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m texto_traduccion \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArtificial intelligence is revolutionizing many industries.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m traduccion \u001b[38;5;241m=\u001b[39m translation_pipe(texto_traduccion)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\__init__.py:931\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    928\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    929\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 931\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:774\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    775\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    776\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    777\u001b[0m             )\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    782\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "\n",
    "translation_pipe = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "texto_traduccion = \"Artificial intelligence is revolutionizing many industries.\"\n",
    "traduccion = translation_pipe(texto_traduccion)\n",
    "\n",
    "print(\"Resultados para 'translation':\")\n",
    "print(traduccion[0]['translation_text'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sentence Similarity (Similitud de oraciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para 'sentence similarity':\n",
      "Similitud entre las oraciones: 0.8508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "similarity_pipe = pipeline(\"feature-extraction\", model=\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "frase1 = \"La inteligencia artificial transformará el futuro.\"\n",
    "frase2 = \"El futuro será transformado por la inteligencia artificial.\"\n",
    "embedding1 = similarity_pipe(frase1)\n",
    "embedding2 = similarity_pipe(frase2)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Calcular la similitud entre las dos oraciones\n",
    "embedding1_mean = np.mean(embedding1, axis=1)\n",
    "embedding2_mean = np.mean(embedding2, axis=1)\n",
    "similarity_score = cosine_similarity(embedding1_mean, embedding2_mean)\n",
    "\n",
    "print(\"Resultados para 'sentence similarity':\")\n",
    "print(f\"Similitud entre las oraciones: {similarity_score[0][0]:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de características con BETO - Convertir a una representación númerica\n",
    "Este notebook demuestra cómo extraer características (embeddings) de un texto en español utilizando el modelo BETO, que es una versión de BERT entrenada en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5200315620e74e07add4a5a80b63d9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b91dfc185d48eeb2f95be301597cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630e78d2081c48cc91f46c702704b7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911304b1c3ae450abdacc606ccb8b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/480k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178f19653eab446285725ba720af29d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e84f8beca104d638af88f75b1d4053a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del embedding del token [CLS]: torch.Size([768])\n",
      "Embeddings para cada token en el texto: torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo y el tokenizador BETO\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Texto en español para la extracción de características\n",
    "text = \"La inteligencia artificial está transformando el mundo.\"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Pasar los tokens por el modelo para obtener los embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extraer los embeddings del último hidden state (última capa del modelo)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Puedes elegir qué token o palabra específica utilizar (por ejemplo, el token CLS que representa el texto completo)\n",
    "cls_embedding = embeddings[0, 0, :]  # Embedding del token [CLS]\n",
    "\n",
    "# Imprimir el tamaño del embedding\n",
    "print(\"Tamaño del embedding del token [CLS]:\", cls_embedding.shape)\n",
    "\n",
    "# Si quieres ver los embeddings para cada palabra en el texto\n",
    "print(\"Embeddings para cada token en el texto:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Crear un pequeño DataFrame con frases en español\n",
    "data = {'frases': [\n",
    "    \"La inteligencia artificial está transformando el mundo.\",\n",
    "    \"Me gusta mucho programar en Python.\",\n",
    "    \"El aprendizaje automático es fascinante.\",\n",
    "    \"La banca está adoptando nuevas tecnologías.\",\n",
    "    \"El análisis de datos es fundamental para la toma de decisiones.\"\n",
    "]}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La inteligencia artificial está transformando ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me gusta mucho programar en Python.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El aprendizaje automático es fascinante.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La banca está adoptando nuevas tecnologías.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El análisis de datos es fundamental para la to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              frases\n",
       "0  La inteligencia artificial está transformando ...\n",
       "1                Me gusta mucho programar en Python.\n",
       "2           El aprendizaje automático es fascinante.\n",
       "3        La banca está adoptando nuevas tecnologías.\n",
       "4  El análisis de datos es fundamental para la to..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo y el tokenizador BETO\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Función para extraer los embeddings del token [CLS] de una frase\n",
    "def extract_features(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]  # Embedding del token [CLS]\n",
    "    return cls_embedding.numpy()\n",
    "\n",
    "# Aplicar la extracción de características a cada frase en el DataFrame\n",
    "df['embeddings'] = df['frases'].apply(lambda x: extract_features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frases</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La inteligencia artificial está transformando ...</td>\n",
       "      <td>[0.08195162, -0.13856784, 0.23574236, -0.18076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me gusta mucho programar en Python.</td>\n",
       "      <td>[0.3350792, -0.7172838, 0.21734935, -0.6204882...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El aprendizaje automático es fascinante.</td>\n",
       "      <td>[0.03380447, -0.24542332, 0.28866348, -0.31269...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La banca está adoptando nuevas tecnologías.</td>\n",
       "      <td>[0.29102713, -0.15174729, 0.16909796, -0.36869...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El análisis de datos es fundamental para la to...</td>\n",
       "      <td>[-0.27688015, -0.010216851, 0.48100483, -0.049...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              frases  \\\n",
       "0  La inteligencia artificial está transformando ...   \n",
       "1                Me gusta mucho programar en Python.   \n",
       "2           El aprendizaje automático es fascinante.   \n",
       "3        La banca está adoptando nuevas tecnologías.   \n",
       "4  El análisis de datos es fundamental para la to...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.08195162, -0.13856784, 0.23574236, -0.18076...  \n",
       "1  [0.3350792, -0.7172838, 0.21734935, -0.6204882...  \n",
       "2  [0.03380447, -0.24542332, 0.28866348, -0.31269...  \n",
       "3  [0.29102713, -0.15174729, 0.16909796, -0.36869...  \n",
       "4  [-0.27688015, -0.010216851, 0.48100483, -0.049...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.stack(df['embeddings'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081952</td>\n",
       "      <td>-0.138568</td>\n",
       "      <td>0.235742</td>\n",
       "      <td>-0.180760</td>\n",
       "      <td>0.449923</td>\n",
       "      <td>0.317921</td>\n",
       "      <td>-0.120947</td>\n",
       "      <td>0.893363</td>\n",
       "      <td>-0.132951</td>\n",
       "      <td>-0.042052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246324</td>\n",
       "      <td>-0.752582</td>\n",
       "      <td>-0.337168</td>\n",
       "      <td>-0.217546</td>\n",
       "      <td>-0.586880</td>\n",
       "      <td>-0.557141</td>\n",
       "      <td>-0.400848</td>\n",
       "      <td>-0.661943</td>\n",
       "      <td>0.035552</td>\n",
       "      <td>-0.278231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335079</td>\n",
       "      <td>-0.717284</td>\n",
       "      <td>0.217349</td>\n",
       "      <td>-0.620488</td>\n",
       "      <td>0.603344</td>\n",
       "      <td>-0.272233</td>\n",
       "      <td>-0.075453</td>\n",
       "      <td>-0.068326</td>\n",
       "      <td>-0.778578</td>\n",
       "      <td>0.054001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674065</td>\n",
       "      <td>-0.190477</td>\n",
       "      <td>-0.597092</td>\n",
       "      <td>0.697127</td>\n",
       "      <td>-1.249478</td>\n",
       "      <td>-0.521828</td>\n",
       "      <td>0.289186</td>\n",
       "      <td>-0.545779</td>\n",
       "      <td>-0.074797</td>\n",
       "      <td>-0.067943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033804</td>\n",
       "      <td>-0.245423</td>\n",
       "      <td>0.288663</td>\n",
       "      <td>-0.312693</td>\n",
       "      <td>0.751276</td>\n",
       "      <td>0.333301</td>\n",
       "      <td>0.152841</td>\n",
       "      <td>-0.141550</td>\n",
       "      <td>-0.491117</td>\n",
       "      <td>0.007622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544840</td>\n",
       "      <td>-0.553562</td>\n",
       "      <td>-0.009548</td>\n",
       "      <td>0.384719</td>\n",
       "      <td>-0.464000</td>\n",
       "      <td>-0.063875</td>\n",
       "      <td>-0.205182</td>\n",
       "      <td>-0.553683</td>\n",
       "      <td>-0.023206</td>\n",
       "      <td>-0.105826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.291027</td>\n",
       "      <td>-0.151747</td>\n",
       "      <td>0.169098</td>\n",
       "      <td>-0.368696</td>\n",
       "      <td>0.423541</td>\n",
       "      <td>0.165202</td>\n",
       "      <td>0.507671</td>\n",
       "      <td>0.356805</td>\n",
       "      <td>-0.572448</td>\n",
       "      <td>-0.111768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320171</td>\n",
       "      <td>-0.666623</td>\n",
       "      <td>-0.330645</td>\n",
       "      <td>0.249393</td>\n",
       "      <td>-0.162970</td>\n",
       "      <td>-0.191086</td>\n",
       "      <td>0.119498</td>\n",
       "      <td>0.246207</td>\n",
       "      <td>-0.119381</td>\n",
       "      <td>0.019641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.276880</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>0.481005</td>\n",
       "      <td>-0.049086</td>\n",
       "      <td>0.020754</td>\n",
       "      <td>-0.303803</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>-0.257698</td>\n",
       "      <td>-0.334976</td>\n",
       "      <td>0.339052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573724</td>\n",
       "      <td>-0.565186</td>\n",
       "      <td>0.334923</td>\n",
       "      <td>0.809215</td>\n",
       "      <td>-0.671297</td>\n",
       "      <td>-0.092552</td>\n",
       "      <td>-0.130470</td>\n",
       "      <td>-0.280019</td>\n",
       "      <td>0.465952</td>\n",
       "      <td>0.657217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.081952 -0.138568  0.235742 -0.180760  0.449923  0.317921 -0.120947   \n",
       "1  0.335079 -0.717284  0.217349 -0.620488  0.603344 -0.272233 -0.075453   \n",
       "2  0.033804 -0.245423  0.288663 -0.312693  0.751276  0.333301  0.152841   \n",
       "3  0.291027 -0.151747  0.169098 -0.368696  0.423541  0.165202  0.507671   \n",
       "4 -0.276880 -0.010217  0.481005 -0.049086  0.020754 -0.303803  0.035176   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.893363 -0.132951 -0.042052  ... -0.246324 -0.752582 -0.337168 -0.217546   \n",
       "1 -0.068326 -0.778578  0.054001  ...  0.674065 -0.190477 -0.597092  0.697127   \n",
       "2 -0.141550 -0.491117  0.007622  ...  0.544840 -0.553562 -0.009548  0.384719   \n",
       "3  0.356805 -0.572448 -0.111768  ... -0.320171 -0.666623 -0.330645  0.249393   \n",
       "4 -0.257698 -0.334976  0.339052  ...  0.573724 -0.565186  0.334923  0.809215   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0 -0.586880 -0.557141 -0.400848 -0.661943  0.035552 -0.278231  \n",
       "1 -1.249478 -0.521828  0.289186 -0.545779 -0.074797 -0.067943  \n",
       "2 -0.464000 -0.063875 -0.205182 -0.553683 -0.023206 -0.105826  \n",
       "3 -0.162970 -0.191086  0.119498  0.246207 -0.119381  0.019641  \n",
       "4 -0.671297 -0.092552 -0.130470 -0.280019  0.465952  0.657217  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicación suave 🫠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación de las dependencias necesarias\n",
    "Primero necesitamos instalar las librerías necesarias para cargar el modelo BETO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar el modelo BETO\n",
    "A continuación, cargamos el modelo `BETO` y su tokenizador desde Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo y el tokenizador BETO\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introducir el texto en español para la extracción de características\n",
    "Vamos a utilizar una frase en español como ejemplo para extraer sus características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens generados: ['[CLS]', 'La', 'inteligencia', 'artificial', 'está', 'transforma', '##n', '##do', 'el', 'mundo', '.', '[SEP]']\n",
      "[4, 1198, 9145, 16061, 1266, 23684, 30935, 1047, 1040, 1863, 1009, 5]\n"
     ]
    }
   ],
   "source": [
    "# Texto en español para la extracción de características\n",
    "text = \"La inteligencia artificial está transformando el mundo.\"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Ver los tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(\"Tokens generados:\", tokens)\n",
    "#Ver los ids\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraer los embeddings del texto tokenizado\n",
    "El modelo procesará los tokens y generará embeddings para cada token en la frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de los embeddings: torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "# Pasar los tokens por el modelo para obtener los embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extraer los embeddings del último hidden state (última capa del modelo)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Mostrar la forma de los embeddings\n",
    "print(\"Forma de los embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extraer el embedding\n",
    "Los embeddings son representaciones numéricas que capturan el significado de las palabras en un espacio multidimensional. Estos embeddings se obtienen de la última capa del modelo y pueden ser utilizados para diversas tareas de NLP.\n",
    "\n",
    "- **Embeddings por palabra**: Cada token tiene un embedding que lo representa en el contexto de la frase.\n",
    "- **Embedding global**: El token `[CLS]` se utiliza como una representación global del significado de toda la frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La longitud máxima que BETO puede procesar es de 512 tokens, por lo que si tu texto excede ese límite, deberías dividirlo en partes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del embedding del token [CLS]: torch.Size([768])\n",
      "Embedding del token [CLS]: tensor([ 8.1952e-02, -1.3857e-01,  2.3574e-01, -1.8076e-01,  4.4992e-01,\n",
      "         3.1792e-01, -1.2095e-01,  8.9336e-01, -1.3295e-01, -4.2052e-02,\n",
      "        -1.3437e-01, -7.5474e-01, -1.9293e-01, -6.6547e-01,  3.9844e-01,\n",
      "        -7.4332e-01, -1.2885e-01,  2.6310e-01, -5.3221e-01,  6.2915e-02,\n",
      "        -3.1543e-01,  6.0861e-01, -7.2414e-02,  4.7553e-01, -8.4074e-01,\n",
      "        -4.6770e-01,  4.7386e-01, -3.2404e-01, -3.1877e-01, -3.0246e-01,\n",
      "         9.5490e-02, -2.2994e-01, -2.0117e-01,  5.6050e-01,  2.4203e-01,\n",
      "        -6.7221e-01,  6.5838e-01,  3.7626e-02,  1.3419e-01, -2.6381e-02,\n",
      "         4.8472e-01,  1.7211e-01,  1.4666e-01,  3.0293e-01, -1.6567e-01,\n",
      "         7.1097e-01,  3.5347e-01,  1.6017e-01,  1.0213e+00, -7.5344e-05,\n",
      "         2.8382e-01, -8.1311e-02, -7.1591e-01, -3.5693e-01,  3.4672e-01,\n",
      "         4.4201e-02, -2.2630e-01, -6.6362e-01, -2.9959e-01, -4.3165e-02,\n",
      "         5.8684e-02, -4.5498e-01, -3.3438e-02, -9.2445e-01,  5.3182e-02,\n",
      "         3.2781e-02, -6.9786e-01,  9.3428e-01, -6.4853e-01, -3.1989e-01,\n",
      "        -1.2679e-01, -5.5881e-01,  3.1727e-01, -1.2155e-02,  4.2472e-01,\n",
      "         1.5822e-01,  4.5566e-01,  6.6387e-01, -4.5191e-01,  8.0794e-02,\n",
      "         2.9296e-01, -9.3741e-02, -1.5329e-01, -4.8499e-02, -8.9441e-01,\n",
      "         1.9240e-01,  4.4343e-01,  2.4193e-01, -2.0820e-02,  4.8754e-01,\n",
      "        -7.8471e-02,  5.9683e-01,  1.6070e-01, -4.9010e-01, -6.7284e-02,\n",
      "        -5.1038e-01,  9.6530e-01,  9.4073e-02, -1.1332e-01,  5.8762e-01,\n",
      "        -2.8135e-01,  3.5217e-01, -5.1060e-02,  5.0515e-01, -2.1765e-02,\n",
      "        -3.0031e-01,  5.6923e-01, -4.1193e+00, -3.8633e-01,  4.1480e-01,\n",
      "         8.3487e-02,  8.0457e-02,  7.5613e-02, -1.8593e-01, -3.9359e-01,\n",
      "        -4.7750e-01,  3.6154e-01,  2.2265e-01,  2.3360e-01,  1.6289e-02,\n",
      "         4.6380e-01,  5.2438e-01, -4.0571e-01,  7.3042e-02,  4.4978e-01,\n",
      "         3.9332e-01, -4.4487e-02,  5.3677e-01, -2.6940e-01, -3.2507e-01,\n",
      "         3.0070e-02, -2.8896e-01, -4.1607e-01, -4.3111e-01,  1.1924e-01,\n",
      "        -2.7382e-01, -4.6283e-01, -5.8010e-01, -2.8895e-01, -7.5808e-02,\n",
      "         5.1726e-01, -1.3545e-01,  2.7609e-01, -4.0639e-01,  1.0266e-01,\n",
      "         1.9915e-01, -7.3446e-02,  1.8214e-01,  8.0529e-01, -7.9337e-02,\n",
      "         1.6854e-01, -1.9134e-01, -3.5789e-01, -2.9715e-01,  8.6968e-03,\n",
      "         7.1751e-01, -3.2530e-01,  2.3251e-02,  1.3006e-01, -3.3401e-01,\n",
      "        -2.3461e-01, -1.5447e-01, -2.1570e-01, -2.3883e-02,  1.4306e-01,\n",
      "        -3.7003e-01,  9.3500e-01, -2.0638e-01, -1.2884e-01, -3.5935e-01,\n",
      "        -4.0839e-01,  1.2637e-01,  8.7069e-02, -4.1554e-01, -1.5276e-01,\n",
      "         5.5021e-01, -1.1663e+00,  3.8975e-01, -8.8998e-01, -2.2077e-01,\n",
      "         2.2650e-02, -2.1982e-01,  1.4547e-01,  1.4401e-01, -8.4674e-01,\n",
      "         5.8944e-01, -8.2156e-03,  1.9124e-01,  6.9476e-03, -9.5310e-01,\n",
      "         4.2769e-01, -4.1380e-01,  3.4061e-01, -1.3560e-01, -4.9367e-01,\n",
      "        -2.7687e-01,  7.8502e-01, -5.4627e-02,  3.0832e-01,  3.8818e-02,\n",
      "        -5.0315e-03,  5.4553e-01, -1.5591e-01,  8.0143e-01,  4.1439e-01,\n",
      "        -5.8709e-01,  1.2291e-02, -3.0140e-01, -3.9320e-01,  2.6593e-01,\n",
      "        -7.2064e+00,  1.9756e-01, -4.9274e-01,  7.2112e-01, -1.1584e-01,\n",
      "        -2.8533e-02,  2.2523e-01,  2.3821e-01,  1.2687e-01,  3.1496e-01,\n",
      "         1.0067e-01,  1.2963e-01, -4.1973e-01, -7.5357e-01, -4.6177e-01,\n",
      "         1.7087e-01,  3.2463e-01, -4.0333e-01, -3.9853e-02,  4.7347e-01,\n",
      "         5.7943e-01, -3.1812e-01, -4.8088e-01, -1.6018e-01, -2.9057e-01,\n",
      "        -7.0653e-01, -3.2870e-01, -2.4664e-01, -5.3545e-01, -5.4535e-01,\n",
      "         2.8219e-01,  6.8214e-01,  4.7262e-01,  3.0092e-01, -1.1497e-01,\n",
      "         1.1454e-02,  2.5434e-01,  2.1249e-01,  6.9067e-01,  3.5059e-01,\n",
      "         3.5746e-01, -5.7033e-02, -1.2577e-01,  7.9680e-01, -1.9232e-01,\n",
      "         1.8947e-01, -7.7319e-02,  4.6998e-01, -3.5895e-01,  6.2659e-01,\n",
      "        -3.0102e-01, -4.2769e-01, -3.0498e-01, -4.7823e-01, -3.1371e-01,\n",
      "         9.6651e-02,  1.9367e-02, -3.7007e-01,  2.4153e-01,  4.4074e-02,\n",
      "         1.4577e-01,  1.3710e-01, -4.9881e-01, -7.3310e-01, -3.1888e-01,\n",
      "         5.0649e-01, -5.7236e-01,  3.0923e-02, -3.6414e-01, -2.8554e-01,\n",
      "         8.1625e-01,  5.4954e-01, -4.7426e-01, -1.4982e-01,  3.8144e-02,\n",
      "        -3.9317e-02, -1.0460e-01, -4.7219e-01,  5.2397e-01, -5.0576e-01,\n",
      "         3.5202e-01, -4.0137e-01,  3.1095e-01, -7.9375e-02,  2.0543e-01,\n",
      "        -1.1642e-01,  2.8765e-01,  4.9252e-01,  2.9916e-01, -4.1969e-01,\n",
      "        -3.4381e-01,  1.7972e-02,  4.4577e-01,  4.4505e-01, -1.4969e-01,\n",
      "        -5.4030e-01,  1.7475e-02, -1.4548e-02,  4.8718e-01,  2.9688e-01,\n",
      "         2.6364e-01,  3.4070e-01,  8.7643e-02, -1.3538e-01, -4.6405e-01,\n",
      "        -3.0432e-02,  6.0542e-01,  6.2298e-03,  5.2618e-01,  5.1252e-01,\n",
      "        -6.0583e-02,  3.2059e-01,  5.0795e-02, -3.7913e-01,  1.1509e-01,\n",
      "         4.5375e-02, -1.3565e-01,  2.1276e-01, -2.3581e-01, -8.1044e-02,\n",
      "        -1.0798e-01, -1.8571e-01, -1.0728e+00,  4.7328e-01, -1.6593e-01,\n",
      "        -1.2072e-01,  2.8705e-01,  9.9581e-02, -4.6817e-02, -3.8153e-01,\n",
      "        -1.1379e+00,  5.4662e-02, -3.9528e-01, -3.1390e-01,  2.3665e-01,\n",
      "         6.9778e-02, -2.0811e-01,  1.2890e-02, -4.6531e-01,  3.2057e-01,\n",
      "         9.4181e-02,  8.6495e-01,  1.6564e-01,  4.8025e-01,  3.7577e-01,\n",
      "        -1.4081e-01, -2.6507e-01, -7.3572e-02, -3.5227e-01, -1.8423e-02,\n",
      "        -1.6227e-01, -6.8800e-01, -3.7778e-01,  6.5797e-01,  5.6698e-01,\n",
      "        -1.6589e-01, -1.2800e-01, -3.0761e-01,  1.0017e-01,  1.1999e-01,\n",
      "         9.2364e-01, -5.6412e-01, -3.4964e-01,  8.6411e-01, -1.1267e-01,\n",
      "        -3.5904e-01,  2.2051e-01, -7.9063e-02, -3.5526e-01, -1.2467e-01,\n",
      "         1.5900e-01, -6.2419e-02, -2.2440e-01,  6.3188e-01,  7.1091e-01,\n",
      "        -2.3875e-01,  2.7848e-01, -2.1440e-01,  2.9097e-01, -3.0688e-01,\n",
      "        -8.6708e-01, -1.7800e-01, -4.2084e-01,  4.2230e-01,  3.0163e-01,\n",
      "         7.1566e-02,  3.7293e-01, -8.7315e-01, -4.0848e-01, -2.7700e-01,\n",
      "         7.3634e-02, -6.3365e-01,  1.7503e-01, -3.5598e-01, -9.9845e-02,\n",
      "         1.9582e-01, -3.4308e-01, -6.8154e-01,  5.8581e-02,  6.3393e-01,\n",
      "        -8.1310e-02, -2.2321e-01, -7.1545e-01, -4.5075e-01, -4.0600e-01,\n",
      "        -6.8357e-01,  2.9135e+00, -4.7896e-01, -1.4925e-01,  3.5409e-01,\n",
      "        -7.7365e-01, -3.3527e-01,  4.5086e-01,  1.4008e-01,  5.1023e-01,\n",
      "        -1.2088e-01, -2.3542e-01,  5.5585e-02,  3.6120e-01,  1.0468e-01,\n",
      "        -1.6335e-01, -3.4525e-01, -1.7934e-01,  9.2136e-02,  4.1637e-01,\n",
      "        -4.0313e-01, -5.4152e-01,  1.7409e-01,  1.1714e-02,  2.3976e-01,\n",
      "         1.7344e-01,  4.4706e-01, -6.2140e-01, -1.0343e-01,  3.3437e-01,\n",
      "         7.8736e-01, -9.1567e-02,  4.8067e-01,  6.5738e-01,  3.6144e-01,\n",
      "         4.8915e-02,  2.1303e-01,  4.0059e-01,  4.9710e-01,  5.0200e-01,\n",
      "         2.6840e-01,  5.0674e-01,  2.3797e-01, -1.2170e-01, -4.7272e-01,\n",
      "        -3.6212e-01,  2.8228e-01, -7.3108e-01, -6.0687e-01,  2.1979e-01,\n",
      "        -1.0977e-01, -5.7985e-01, -7.9890e-01, -5.1609e-01,  2.9611e-01,\n",
      "         8.7503e-02,  2.4944e-01, -2.1890e-02,  2.1669e-01,  5.2178e-01,\n",
      "        -4.2051e-01,  3.6763e-01,  7.9214e-01,  1.3569e-01, -1.5177e-01,\n",
      "        -3.5250e-01,  2.5388e-01, -6.2935e-01,  1.3537e-01,  7.6654e-01,\n",
      "        -5.3259e-01,  8.1554e-01,  5.6624e-02,  1.6385e-01,  3.0743e-01,\n",
      "        -5.7660e-01,  1.5037e-01,  1.5071e-01,  7.2490e-01,  3.9679e-01,\n",
      "        -8.0009e-01,  8.8988e-01, -6.4599e-01, -1.1653e-01,  5.3490e-01,\n",
      "        -4.2875e-02, -6.3929e-01,  2.5601e-01,  1.9401e-01,  7.6371e-02,\n",
      "        -6.6514e-01, -3.0451e-01, -6.4938e-02, -1.4087e-01,  1.2739e-01,\n",
      "        -1.3538e-01,  2.3661e-01,  9.4390e-01,  5.7977e-01,  1.4938e-02,\n",
      "        -2.0471e-02,  3.2937e-01,  2.9697e-01,  2.9512e-01,  1.8669e-01,\n",
      "         1.5200e-01,  1.6643e-01,  2.6421e-01, -4.1812e-01,  8.4733e-01,\n",
      "        -2.7933e-01,  6.9710e-02, -2.7250e-01, -8.2575e-01,  1.3175e-01,\n",
      "         1.0234e-01,  7.5936e-02,  7.1978e-01, -4.0852e-01,  1.1842e-01,\n",
      "         2.3656e-01,  5.0329e-01, -3.8426e-01, -2.4022e-01, -5.0387e-03,\n",
      "         1.2979e+00, -2.8820e-01,  1.6939e-01, -7.7244e-03,  8.3645e-02,\n",
      "         1.2011e-01,  8.5828e-02, -6.2860e-01,  1.2834e-01, -5.6821e-01,\n",
      "         2.0169e-01,  2.3180e-01,  7.7567e-01,  3.2506e-01,  6.3621e-02,\n",
      "         1.9635e-01, -4.1105e-01, -1.3785e-01, -1.2672e+01, -6.0461e-01,\n",
      "         5.4662e-01, -1.6453e-01, -4.1747e-01, -1.1156e-01,  2.5894e-01,\n",
      "         2.2227e-02, -3.5365e-01, -1.8938e-01,  1.1660e-02,  7.1135e-01,\n",
      "         9.1709e-01,  9.9433e-02, -8.2206e-01, -1.4940e-01,  6.9083e-02,\n",
      "         7.9197e-01,  2.0849e-01,  1.8751e-01, -5.0361e-01,  1.3976e-01,\n",
      "        -2.1762e-01, -5.2727e-01,  7.2799e-01,  2.5814e-01,  1.3059e-01,\n",
      "        -3.5964e-01, -3.7727e-01,  3.6870e-01,  5.5320e-01, -4.5154e-01,\n",
      "        -2.4548e-01, -5.4487e-01,  2.2874e-03, -1.3633e-01, -3.9987e-01,\n",
      "        -9.6901e-02,  2.4240e-01,  7.0434e-01, -1.2333e-01, -8.9731e-02,\n",
      "         2.2859e-01,  1.6612e-01, -1.8512e-01,  5.6885e-01,  2.0100e-01,\n",
      "         3.6616e-01,  1.5317e-01,  6.9105e-01,  1.9583e-02,  3.7611e-01,\n",
      "         1.0312e+00, -2.1457e-01, -3.9483e-01, -1.4665e-01,  6.2577e-01,\n",
      "        -1.8515e-01, -3.1915e-01, -6.2350e-01,  7.8260e-02, -2.1850e-01,\n",
      "         9.5003e-01,  2.0913e-01, -4.2788e-02, -7.6636e-01, -5.6977e-01,\n",
      "        -5.2268e-01, -1.2830e-01, -8.4175e-02,  4.1396e-01, -9.0383e-01,\n",
      "        -1.2172e-01, -3.2099e-01, -5.3960e-01, -1.7603e-01, -8.7361e-01,\n",
      "        -4.2969e-01, -7.5813e-01,  4.2659e-01,  3.9066e-01, -1.3467e+00,\n",
      "         1.7811e-01,  2.9514e-02,  3.8213e-01, -1.8032e-01, -5.3158e-01,\n",
      "        -4.9084e-01, -6.5023e-01,  6.8821e-02,  2.7723e-01, -2.2008e-01,\n",
      "        -1.7570e-01,  7.8770e-02,  2.6018e-01, -1.1979e-01,  5.0020e-01,\n",
      "         5.5483e-01,  7.3787e-02,  5.2920e-02,  3.7347e-01, -9.2976e-02,\n",
      "         5.1423e-02, -2.0620e-01,  3.9870e-01,  1.0556e-02, -3.0127e-01,\n",
      "         8.0947e-01, -8.8483e-02, -2.6117e-01,  2.0419e-01,  1.5946e-01,\n",
      "         7.7608e-01,  4.0682e-02, -2.3621e-01, -6.3481e-01,  1.7103e-01,\n",
      "        -4.3413e-01,  4.0318e-01,  5.0623e-01, -1.3230e-01,  6.7810e-01,\n",
      "        -3.3445e-01,  1.4430e-01, -3.3927e-01, -3.1590e-01, -3.3338e-01,\n",
      "         2.1299e-01,  3.3015e-02,  3.1188e-01, -3.4024e-01, -1.4752e-01,\n",
      "        -4.1744e-01,  6.6795e-02, -1.0811e-01,  3.1713e-02, -2.9733e-01,\n",
      "        -3.8682e-01, -2.0771e-02,  1.7287e-01,  2.7052e-02, -4.8108e-01,\n",
      "         1.9969e-01,  2.0850e-02, -8.6351e-02,  2.4049e-02, -7.2054e-01,\n",
      "        -2.4475e-01, -1.7721e-01,  4.2834e-01,  2.3319e-02, -2.9296e-01,\n",
      "         5.6954e-01,  6.4165e-01,  7.1479e-01,  1.8394e-01,  7.7559e-03,\n",
      "        -3.5321e-01,  1.5476e-01, -5.5245e-01, -2.1073e-01, -2.9440e-02,\n",
      "        -3.6585e-01, -5.1121e-02, -4.5107e-01,  1.5458e-01, -6.7454e-01,\n",
      "         1.2202e-01, -2.2382e-01,  6.6349e-01, -6.2558e-01,  6.4152e-01,\n",
      "        -1.4675e-01,  1.9345e-01,  2.4783e-01,  3.3400e-01,  3.7997e-01,\n",
      "         8.9359e-01,  1.4917e-01, -6.5576e-01,  8.7905e-02, -9.6151e-02,\n",
      "         1.2556e-01,  4.8867e-01, -1.0314e-01,  2.7125e-01,  2.7728e-01,\n",
      "        -2.5051e-01, -1.8205e-01,  1.5419e-01, -2.4971e-01,  6.2710e-01,\n",
      "         2.2956e-01,  4.2598e-01,  4.4557e-01, -7.8941e-01, -1.7454e-01,\n",
      "        -2.7174e-02,  6.0686e-01,  2.9939e-01, -2.4632e-01, -7.5258e-01,\n",
      "        -3.3717e-01, -2.1755e-01, -5.8688e-01, -5.5714e-01, -4.0085e-01,\n",
      "        -6.6194e-01,  3.5552e-02, -2.7823e-01])\n"
     ]
    }
   ],
   "source": [
    "# Embedding del token [CLS] que representa la frase completa\n",
    "cls_embedding = embeddings[0, 0, :]  # El primer [0] es la primera secuencia, el segundo [0] es el token [CLS]\n",
    "\n",
    "# Mostrar el tamaño del embedding del token [CLS]\n",
    "print(\"Tamaño del embedding del token [CLS]:\", cls_embedding.shape)\n",
    "print(\"Embedding del token [CLS]:\", cls_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embeddings por palabra\n",
    "También podemos ver los embeddings para cada token de la frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS] -> Embedding: tensor([ 0.0820, -0.1386,  0.2357, -0.1808,  0.4499])... (truncado)\n",
      "Token: La -> Embedding: tensor([ 0.0681,  0.6668, -0.7685, -0.2641,  0.3665])... (truncado)\n",
      "Token: inteligencia -> Embedding: tensor([-0.1536, -0.4845, -0.4437,  0.1451, -0.2135])... (truncado)\n",
      "Token: artificial -> Embedding: tensor([-0.5083, -0.5315,  0.1694, -0.4105,  0.2094])... (truncado)\n",
      "Token: está -> Embedding: tensor([-0.1063,  0.2039,  0.0624, -0.3050,  0.4447])... (truncado)\n",
      "Token: transforma -> Embedding: tensor([-0.4565,  0.3401, -0.7017,  0.3512, -0.0459])... (truncado)\n",
      "Token: ##n -> Embedding: tensor([-0.5090,  0.5637, -0.9005,  0.3523,  0.3229])... (truncado)\n",
      "Token: ##do -> Embedding: tensor([-0.4623,  0.4462, -1.0330,  0.2840,  0.3256])... (truncado)\n",
      "Token: el -> Embedding: tensor([-0.8480,  0.8487, -0.4200,  0.7275, -0.1490])... (truncado)\n",
      "Token: mundo -> Embedding: tensor([-0.5589,  0.2899, -0.1544,  0.3578, -0.2566])... (truncado)\n",
      "Token: . -> Embedding: tensor([-0.3829,  0.3332,  0.1027, -0.0988,  0.2425])... (truncado)\n",
      "Token: [SEP] -> Embedding: tensor([-1.4672, -0.5293,  0.0109,  1.0246, -0.4454])... (truncado)\n"
     ]
    }
   ],
   "source": [
    "# Embeddings para cada token en la frase\n",
    "for token, embedding in zip(tokens, embeddings[0]):\n",
    "    print(f\"Token: {token} -> Embedding: {embedding[:5]}... (truncado)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

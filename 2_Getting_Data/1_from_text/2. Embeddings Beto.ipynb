{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fill-Mask (Completado de texto con m√°scara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para 'fill-mask':\n",
      "Palabra predicha: necesario | Score: 0.1978\n",
      "Palabra predicha: bueno | Score: 0.1463\n",
      "Palabra predicha: esencial | Score: 0.1131\n",
      "Palabra predicha: importante | Score: 0.0878\n",
      "Palabra predicha: vital | Score: 0.0630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fill_mask_pipe = pipeline(\"fill-mask\", model=\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "frase_fill_mask = \"El cambio clim√°tico es [MASK] para el futuro.\"\n",
    "resultados_fill_mask = fill_mask_pipe(frase_fill_mask)\n",
    "\n",
    "print(\"Resultados para 'fill-mask':\")\n",
    "for res in resultados_fill_mask:\n",
    "    print(f\"Palabra predicha: {res['token_str']} | Score: {res['score']:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Classification (Clasificaci√≥n de texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza para clasificar el texto. En este ejemplo, se clasifica el sentimiento de un texto como positivo, negativo o neutral.\n",
    "1 estrella: Muy negativo.\n",
    "2 estrellas: Negativo.\n",
    "3 estrellas: Neutro.\n",
    "4 estrellas: Positivo.\n",
    "5 estrellas: Muy positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para 'text-classification':\n",
      "[{'label': '5 stars', 'score': 0.8085700869560242}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_class_pipe = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "texto_clasificacion = \"Este producto es excelente y de alta calidad.\"\n",
    "resultado_clasificacion = text_class_pipe(texto_clasificacion)\n",
    "\n",
    "print(\"Resultados para 'text-classification':\")\n",
    "print(resultado_clasificacion)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Token Classification (Clasificaci√≥n por token / NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realiza reconocimiento de entidades nombradas (NER), etiquetando palabras como organizaciones, personas, lugares, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para 'token-classification' (NER):\n",
      "Entidad: Apple | Tipo: B-ORG | Score: 0.9971\n",
      "Entidad: Bogot√° | Tipo: B-LOC | Score: 0.9998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_class_pipe = pipeline(\"token-classification\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\")\n",
    "frase_token_class = \"Apple est√° abriendo una nueva oficina en Bogot√°.\"\n",
    "resultados_token_class = token_class_pipe(frase_token_class)\n",
    "\n",
    "print(\"Resultados para 'token-classification' (NER):\")\n",
    "for res in resultados_token_class:\n",
    "    print(f\"Entidad: {res['word']} | Tipo: {res['entity']} | Score: {res['score']:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Generation (Generaci√≥n de texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para 'text-generation':\n",
      "La inteligencia artificial es una tecnolog√≠a no invasiva que combina el control de campo, la detecci√≥n de objetos y habilidades de sensores, y la capacidad de identificar y detectar objetos muy peque√±os. Adem√°s, al control del campo se le hace un sistema de comunicaci√≥n de\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_gen_pipe = pipeline(\"text-generation\", model=\"datificate/gpt2-small-spanish\")\n",
    "frase_inicial_text_gen = \"La inteligencia artificial es\"\n",
    "resultado_text_gen = text_gen_pipe(frase_inicial_text_gen, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"Resultados para 'text-generation':\")\n",
    "for res in resultado_text_gen:\n",
    "    print(res['generated_text'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summarization (Resumen de texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_pipe = pipeline(\"summarization\", model=\"mrm8488/t5-base-finetuned-summarize-news\")\n",
    "texto_largo_summarization = \"\"\"\n",
    "La inteligencia artificial est√° transformando muchas industrias. Empresas como Google, Microsoft, y Amazon est√°n invirtiendo en \n",
    "nuevas tecnolog√≠as para mejorar la eficiencia y la toma de decisiones automatizada. Estas innovaciones podr√≠an cambiar la forma \n",
    "en que trabajamos y vivimos.\n",
    "\"\"\"\n",
    "resumen = summarization_pipe(texto_largo_summarization)\n",
    "\n",
    "print(\"Resultados para 'summarization':\")\n",
    "print(resumen[0]['summary_text'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Translation (Traducci√≥n de texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ddfff36e1e472a89291aef415babf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773e42d6ced24cbd856f349699f069f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbef08cce34347bdab7aa950829c1060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1ba25b9b4b498d8797a90e8453cde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m translation_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation_en_to_es\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHelsinki-NLP/opus-mt-en-es\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m texto_traduccion \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArtificial intelligence is revolutionizing many industries.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m traduccion \u001b[38;5;241m=\u001b[39m translation_pipe(texto_traduccion)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\pipelines\\__init__.py:931\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    928\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    929\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 931\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:774\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    775\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    776\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    777\u001b[0m             )\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    782\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "\n",
    "translation_pipe = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "texto_traduccion = \"Artificial intelligence is revolutionizing many industries.\"\n",
    "traduccion = translation_pipe(texto_traduccion)\n",
    "\n",
    "print(\"Resultados para 'translation':\")\n",
    "print(traduccion[0]['translation_text'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sentence Similarity (Similitud de oraciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para 'sentence similarity':\n",
      "Similitud entre las oraciones: 0.8508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "similarity_pipe = pipeline(\"feature-extraction\", model=\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "frase1 = \"La inteligencia artificial transformar√° el futuro.\"\n",
    "frase2 = \"El futuro ser√° transformado por la inteligencia artificial.\"\n",
    "embedding1 = similarity_pipe(frase1)\n",
    "embedding2 = similarity_pipe(frase2)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Calcular la similitud entre las dos oraciones\n",
    "embedding1_mean = np.mean(embedding1, axis=1)\n",
    "embedding2_mean = np.mean(embedding2, axis=1)\n",
    "similarity_score = cosine_similarity(embedding1_mean, embedding2_mean)\n",
    "\n",
    "print(\"Resultados para 'sentence similarity':\")\n",
    "print(f\"Similitud entre las oraciones: {similarity_score[0][0]:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracci√≥n de caracter√≠sticas con BETO - Convertir a una representaci√≥n n√∫merica\n",
    "Este notebook demuestra c√≥mo extraer caracter√≠sticas (embeddings) de un texto en espa√±ol utilizando el modelo BETO, que es una versi√≥n de BERT entrenada en espa√±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5200315620e74e07add4a5a80b63d9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b91dfc185d48eeb2f95be301597cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630e78d2081c48cc91f46c702704b7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911304b1c3ae450abdacc606ccb8b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/480k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178f19653eab446285725ba720af29d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e84f8beca104d638af88f75b1d4053a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del embedding del token [CLS]: torch.Size([768])\n",
      "Embeddings para cada token en el texto: torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo y el tokenizador BETO\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Texto en espa√±ol para la extracci√≥n de caracter√≠sticas\n",
    "text = \"La inteligencia artificial est√° transformando el mundo.\"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Pasar los tokens por el modelo para obtener los embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extraer los embeddings del √∫ltimo hidden state (√∫ltima capa del modelo)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Puedes elegir qu√© token o palabra espec√≠fica utilizar (por ejemplo, el token CLS que representa el texto completo)\n",
    "cls_embedding = embeddings[0, 0, :]  # Embedding del token [CLS]\n",
    "\n",
    "# Imprimir el tama√±o del embedding\n",
    "print(\"Tama√±o del embedding del token [CLS]:\", cls_embedding.shape)\n",
    "\n",
    "# Si quieres ver los embeddings para cada palabra en el texto\n",
    "print(\"Embeddings para cada token en el texto:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Crear un peque√±o DataFrame con frases en espa√±ol\n",
    "data = {'frases': [\n",
    "    \"La inteligencia artificial est√° transformando el mundo.\",\n",
    "    \"Me gusta mucho programar en Python.\",\n",
    "    \"El aprendizaje autom√°tico es fascinante.\",\n",
    "    \"La banca est√° adoptando nuevas tecnolog√≠as.\",\n",
    "    \"El an√°lisis de datos es fundamental para la toma de decisiones.\"\n",
    "]}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La inteligencia artificial est√° transformando ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me gusta mucho programar en Python.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El aprendizaje autom√°tico es fascinante.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La banca est√° adoptando nuevas tecnolog√≠as.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El an√°lisis de datos es fundamental para la to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              frases\n",
       "0  La inteligencia artificial est√° transformando ...\n",
       "1                Me gusta mucho programar en Python.\n",
       "2           El aprendizaje autom√°tico es fascinante.\n",
       "3        La banca est√° adoptando nuevas tecnolog√≠as.\n",
       "4  El an√°lisis de datos es fundamental para la to..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo y el tokenizador BETO\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Funci√≥n para extraer los embeddings del token [CLS] de una frase\n",
    "def extract_features(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]  # Embedding del token [CLS]\n",
    "    return cls_embedding.numpy()\n",
    "\n",
    "# Aplicar la extracci√≥n de caracter√≠sticas a cada frase en el DataFrame\n",
    "df['embeddings'] = df['frases'].apply(lambda x: extract_features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frases</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La inteligencia artificial est√° transformando ...</td>\n",
       "      <td>[0.08195162, -0.13856784, 0.23574236, -0.18076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me gusta mucho programar en Python.</td>\n",
       "      <td>[0.3350792, -0.7172838, 0.21734935, -0.6204882...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El aprendizaje autom√°tico es fascinante.</td>\n",
       "      <td>[0.03380447, -0.24542332, 0.28866348, -0.31269...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La banca est√° adoptando nuevas tecnolog√≠as.</td>\n",
       "      <td>[0.29102713, -0.15174729, 0.16909796, -0.36869...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El an√°lisis de datos es fundamental para la to...</td>\n",
       "      <td>[-0.27688015, -0.010216851, 0.48100483, -0.049...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              frases  \\\n",
       "0  La inteligencia artificial est√° transformando ...   \n",
       "1                Me gusta mucho programar en Python.   \n",
       "2           El aprendizaje autom√°tico es fascinante.   \n",
       "3        La banca est√° adoptando nuevas tecnolog√≠as.   \n",
       "4  El an√°lisis de datos es fundamental para la to...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.08195162, -0.13856784, 0.23574236, -0.18076...  \n",
       "1  [0.3350792, -0.7172838, 0.21734935, -0.6204882...  \n",
       "2  [0.03380447, -0.24542332, 0.28866348, -0.31269...  \n",
       "3  [0.29102713, -0.15174729, 0.16909796, -0.36869...  \n",
       "4  [-0.27688015, -0.010216851, 0.48100483, -0.049...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.stack(df['embeddings'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081952</td>\n",
       "      <td>-0.138568</td>\n",
       "      <td>0.235742</td>\n",
       "      <td>-0.180760</td>\n",
       "      <td>0.449923</td>\n",
       "      <td>0.317921</td>\n",
       "      <td>-0.120947</td>\n",
       "      <td>0.893363</td>\n",
       "      <td>-0.132951</td>\n",
       "      <td>-0.042052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246324</td>\n",
       "      <td>-0.752582</td>\n",
       "      <td>-0.337168</td>\n",
       "      <td>-0.217546</td>\n",
       "      <td>-0.586880</td>\n",
       "      <td>-0.557141</td>\n",
       "      <td>-0.400848</td>\n",
       "      <td>-0.661943</td>\n",
       "      <td>0.035552</td>\n",
       "      <td>-0.278231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335079</td>\n",
       "      <td>-0.717284</td>\n",
       "      <td>0.217349</td>\n",
       "      <td>-0.620488</td>\n",
       "      <td>0.603344</td>\n",
       "      <td>-0.272233</td>\n",
       "      <td>-0.075453</td>\n",
       "      <td>-0.068326</td>\n",
       "      <td>-0.778578</td>\n",
       "      <td>0.054001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674065</td>\n",
       "      <td>-0.190477</td>\n",
       "      <td>-0.597092</td>\n",
       "      <td>0.697127</td>\n",
       "      <td>-1.249478</td>\n",
       "      <td>-0.521828</td>\n",
       "      <td>0.289186</td>\n",
       "      <td>-0.545779</td>\n",
       "      <td>-0.074797</td>\n",
       "      <td>-0.067943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033804</td>\n",
       "      <td>-0.245423</td>\n",
       "      <td>0.288663</td>\n",
       "      <td>-0.312693</td>\n",
       "      <td>0.751276</td>\n",
       "      <td>0.333301</td>\n",
       "      <td>0.152841</td>\n",
       "      <td>-0.141550</td>\n",
       "      <td>-0.491117</td>\n",
       "      <td>0.007622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544840</td>\n",
       "      <td>-0.553562</td>\n",
       "      <td>-0.009548</td>\n",
       "      <td>0.384719</td>\n",
       "      <td>-0.464000</td>\n",
       "      <td>-0.063875</td>\n",
       "      <td>-0.205182</td>\n",
       "      <td>-0.553683</td>\n",
       "      <td>-0.023206</td>\n",
       "      <td>-0.105826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.291027</td>\n",
       "      <td>-0.151747</td>\n",
       "      <td>0.169098</td>\n",
       "      <td>-0.368696</td>\n",
       "      <td>0.423541</td>\n",
       "      <td>0.165202</td>\n",
       "      <td>0.507671</td>\n",
       "      <td>0.356805</td>\n",
       "      <td>-0.572448</td>\n",
       "      <td>-0.111768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320171</td>\n",
       "      <td>-0.666623</td>\n",
       "      <td>-0.330645</td>\n",
       "      <td>0.249393</td>\n",
       "      <td>-0.162970</td>\n",
       "      <td>-0.191086</td>\n",
       "      <td>0.119498</td>\n",
       "      <td>0.246207</td>\n",
       "      <td>-0.119381</td>\n",
       "      <td>0.019641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.276880</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>0.481005</td>\n",
       "      <td>-0.049086</td>\n",
       "      <td>0.020754</td>\n",
       "      <td>-0.303803</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>-0.257698</td>\n",
       "      <td>-0.334976</td>\n",
       "      <td>0.339052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573724</td>\n",
       "      <td>-0.565186</td>\n",
       "      <td>0.334923</td>\n",
       "      <td>0.809215</td>\n",
       "      <td>-0.671297</td>\n",
       "      <td>-0.092552</td>\n",
       "      <td>-0.130470</td>\n",
       "      <td>-0.280019</td>\n",
       "      <td>0.465952</td>\n",
       "      <td>0.657217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.081952 -0.138568  0.235742 -0.180760  0.449923  0.317921 -0.120947   \n",
       "1  0.335079 -0.717284  0.217349 -0.620488  0.603344 -0.272233 -0.075453   \n",
       "2  0.033804 -0.245423  0.288663 -0.312693  0.751276  0.333301  0.152841   \n",
       "3  0.291027 -0.151747  0.169098 -0.368696  0.423541  0.165202  0.507671   \n",
       "4 -0.276880 -0.010217  0.481005 -0.049086  0.020754 -0.303803  0.035176   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.893363 -0.132951 -0.042052  ... -0.246324 -0.752582 -0.337168 -0.217546   \n",
       "1 -0.068326 -0.778578  0.054001  ...  0.674065 -0.190477 -0.597092  0.697127   \n",
       "2 -0.141550 -0.491117  0.007622  ...  0.544840 -0.553562 -0.009548  0.384719   \n",
       "3  0.356805 -0.572448 -0.111768  ... -0.320171 -0.666623 -0.330645  0.249393   \n",
       "4 -0.257698 -0.334976  0.339052  ...  0.573724 -0.565186  0.334923  0.809215   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0 -0.586880 -0.557141 -0.400848 -0.661943  0.035552 -0.278231  \n",
       "1 -1.249478 -0.521828  0.289186 -0.545779 -0.074797 -0.067943  \n",
       "2 -0.464000 -0.063875 -0.205182 -0.553683 -0.023206 -0.105826  \n",
       "3 -0.162970 -0.191086  0.119498  0.246207 -0.119381  0.019641  \n",
       "4 -0.671297 -0.092552 -0.130470 -0.280019  0.465952  0.657217  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicaci√≥n suave ü´†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n de las dependencias necesarias\n",
    "Primero necesitamos instalar las librer√≠as necesarias para cargar el modelo BETO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar el modelo BETO\n",
    "A continuaci√≥n, cargamos el modelo `BETO` y su tokenizador desde Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo y el tokenizador BETO\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introducir el texto en espa√±ol para la extracci√≥n de caracter√≠sticas\n",
    "Vamos a utilizar una frase en espa√±ol como ejemplo para extraer sus caracter√≠sticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens generados: ['[CLS]', 'La', 'inteligencia', 'artificial', 'est√°', 'transforma', '##n', '##do', 'el', 'mundo', '.', '[SEP]']\n",
      "[4, 1198, 9145, 16061, 1266, 23684, 30935, 1047, 1040, 1863, 1009, 5]\n"
     ]
    }
   ],
   "source": [
    "# Texto en espa√±ol para la extracci√≥n de caracter√≠sticas\n",
    "text = \"La inteligencia artificial est√° transformando el mundo.\"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Ver los tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "print(\"Tokens generados:\", tokens)\n",
    "#Ver los ids\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraer los embeddings del texto tokenizado\n",
    "El modelo procesar√° los tokens y generar√° embeddings para cada token en la frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de los embeddings: torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "# Pasar los tokens por el modelo para obtener los embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extraer los embeddings del √∫ltimo hidden state (√∫ltima capa del modelo)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Mostrar la forma de los embeddings\n",
    "print(\"Forma de los embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extraer el embedding\n",
    "Los embeddings son representaciones num√©ricas que capturan el significado de las palabras en un espacio multidimensional. Estos embeddings se obtienen de la √∫ltima capa del modelo y pueden ser utilizados para diversas tareas de NLP.\n",
    "\n",
    "- **Embeddings por palabra**: Cada token tiene un embedding que lo representa en el contexto de la frase.\n",
    "- **Embedding global**: El token `[CLS]` se utiliza como una representaci√≥n global del significado de toda la frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La longitud m√°xima que BETO puede procesar es de 512 tokens, por lo que si tu texto excede ese l√≠mite, deber√≠as dividirlo en partes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del embedding del token [CLS]: torch.Size([768])\n",
      "Embedding del token [CLS]: tensor([ 8.1952e-02, -1.3857e-01,  2.3574e-01, -1.8076e-01,  4.4992e-01,\n",
      "         3.1792e-01, -1.2095e-01,  8.9336e-01, -1.3295e-01, -4.2052e-02,\n",
      "        -1.3437e-01, -7.5474e-01, -1.9293e-01, -6.6547e-01,  3.9844e-01,\n",
      "        -7.4332e-01, -1.2885e-01,  2.6310e-01, -5.3221e-01,  6.2915e-02,\n",
      "        -3.1543e-01,  6.0861e-01, -7.2414e-02,  4.7553e-01, -8.4074e-01,\n",
      "        -4.6770e-01,  4.7386e-01, -3.2404e-01, -3.1877e-01, -3.0246e-01,\n",
      "         9.5490e-02, -2.2994e-01, -2.0117e-01,  5.6050e-01,  2.4203e-01,\n",
      "        -6.7221e-01,  6.5838e-01,  3.7626e-02,  1.3419e-01, -2.6381e-02,\n",
      "         4.8472e-01,  1.7211e-01,  1.4666e-01,  3.0293e-01, -1.6567e-01,\n",
      "         7.1097e-01,  3.5347e-01,  1.6017e-01,  1.0213e+00, -7.5344e-05,\n",
      "         2.8382e-01, -8.1311e-02, -7.1591e-01, -3.5693e-01,  3.4672e-01,\n",
      "         4.4201e-02, -2.2630e-01, -6.6362e-01, -2.9959e-01, -4.3165e-02,\n",
      "         5.8684e-02, -4.5498e-01, -3.3438e-02, -9.2445e-01,  5.3182e-02,\n",
      "         3.2781e-02, -6.9786e-01,  9.3428e-01, -6.4853e-01, -3.1989e-01,\n",
      "        -1.2679e-01, -5.5881e-01,  3.1727e-01, -1.2155e-02,  4.2472e-01,\n",
      "         1.5822e-01,  4.5566e-01,  6.6387e-01, -4.5191e-01,  8.0794e-02,\n",
      "         2.9296e-01, -9.3741e-02, -1.5329e-01, -4.8499e-02, -8.9441e-01,\n",
      "         1.9240e-01,  4.4343e-01,  2.4193e-01, -2.0820e-02,  4.8754e-01,\n",
      "        -7.8471e-02,  5.9683e-01,  1.6070e-01, -4.9010e-01, -6.7284e-02,\n",
      "        -5.1038e-01,  9.6530e-01,  9.4073e-02, -1.1332e-01,  5.8762e-01,\n",
      "        -2.8135e-01,  3.5217e-01, -5.1060e-02,  5.0515e-01, -2.1765e-02,\n",
      "        -3.0031e-01,  5.6923e-01, -4.1193e+00, -3.8633e-01,  4.1480e-01,\n",
      "         8.3487e-02,  8.0457e-02,  7.5613e-02, -1.8593e-01, -3.9359e-01,\n",
      "        -4.7750e-01,  3.6154e-01,  2.2265e-01,  2.3360e-01,  1.6289e-02,\n",
      "         4.6380e-01,  5.2438e-01, -4.0571e-01,  7.3042e-02,  4.4978e-01,\n",
      "         3.9332e-01, -4.4487e-02,  5.3677e-01, -2.6940e-01, -3.2507e-01,\n",
      "         3.0070e-02, -2.8896e-01, -4.1607e-01, -4.3111e-01,  1.1924e-01,\n",
      "        -2.7382e-01, -4.6283e-01, -5.8010e-01, -2.8895e-01, -7.5808e-02,\n",
      "         5.1726e-01, -1.3545e-01,  2.7609e-01, -4.0639e-01,  1.0266e-01,\n",
      "         1.9915e-01, -7.3446e-02,  1.8214e-01,  8.0529e-01, -7.9337e-02,\n",
      "         1.6854e-01, -1.9134e-01, -3.5789e-01, -2.9715e-01,  8.6968e-03,\n",
      "         7.1751e-01, -3.2530e-01,  2.3251e-02,  1.3006e-01, -3.3401e-01,\n",
      "        -2.3461e-01, -1.5447e-01, -2.1570e-01, -2.3883e-02,  1.4306e-01,\n",
      "        -3.7003e-01,  9.3500e-01, -2.0638e-01, -1.2884e-01, -3.5935e-01,\n",
      "        -4.0839e-01,  1.2637e-01,  8.7069e-02, -4.1554e-01, -1.5276e-01,\n",
      "         5.5021e-01, -1.1663e+00,  3.8975e-01, -8.8998e-01, -2.2077e-01,\n",
      "         2.2650e-02, -2.1982e-01,  1.4547e-01,  1.4401e-01, -8.4674e-01,\n",
      "         5.8944e-01, -8.2156e-03,  1.9124e-01,  6.9476e-03, -9.5310e-01,\n",
      "         4.2769e-01, -4.1380e-01,  3.4061e-01, -1.3560e-01, -4.9367e-01,\n",
      "        -2.7687e-01,  7.8502e-01, -5.4627e-02,  3.0832e-01,  3.8818e-02,\n",
      "        -5.0315e-03,  5.4553e-01, -1.5591e-01,  8.0143e-01,  4.1439e-01,\n",
      "        -5.8709e-01,  1.2291e-02, -3.0140e-01, -3.9320e-01,  2.6593e-01,\n",
      "        -7.2064e+00,  1.9756e-01, -4.9274e-01,  7.2112e-01, -1.1584e-01,\n",
      "        -2.8533e-02,  2.2523e-01,  2.3821e-01,  1.2687e-01,  3.1496e-01,\n",
      "         1.0067e-01,  1.2963e-01, -4.1973e-01, -7.5357e-01, -4.6177e-01,\n",
      "         1.7087e-01,  3.2463e-01, -4.0333e-01, -3.9853e-02,  4.7347e-01,\n",
      "         5.7943e-01, -3.1812e-01, -4.8088e-01, -1.6018e-01, -2.9057e-01,\n",
      "        -7.0653e-01, -3.2870e-01, -2.4664e-01, -5.3545e-01, -5.4535e-01,\n",
      "         2.8219e-01,  6.8214e-01,  4.7262e-01,  3.0092e-01, -1.1497e-01,\n",
      "         1.1454e-02,  2.5434e-01,  2.1249e-01,  6.9067e-01,  3.5059e-01,\n",
      "         3.5746e-01, -5.7033e-02, -1.2577e-01,  7.9680e-01, -1.9232e-01,\n",
      "         1.8947e-01, -7.7319e-02,  4.6998e-01, -3.5895e-01,  6.2659e-01,\n",
      "        -3.0102e-01, -4.2769e-01, -3.0498e-01, -4.7823e-01, -3.1371e-01,\n",
      "         9.6651e-02,  1.9367e-02, -3.7007e-01,  2.4153e-01,  4.4074e-02,\n",
      "         1.4577e-01,  1.3710e-01, -4.9881e-01, -7.3310e-01, -3.1888e-01,\n",
      "         5.0649e-01, -5.7236e-01,  3.0923e-02, -3.6414e-01, -2.8554e-01,\n",
      "         8.1625e-01,  5.4954e-01, -4.7426e-01, -1.4982e-01,  3.8144e-02,\n",
      "        -3.9317e-02, -1.0460e-01, -4.7219e-01,  5.2397e-01, -5.0576e-01,\n",
      "         3.5202e-01, -4.0137e-01,  3.1095e-01, -7.9375e-02,  2.0543e-01,\n",
      "        -1.1642e-01,  2.8765e-01,  4.9252e-01,  2.9916e-01, -4.1969e-01,\n",
      "        -3.4381e-01,  1.7972e-02,  4.4577e-01,  4.4505e-01, -1.4969e-01,\n",
      "        -5.4030e-01,  1.7475e-02, -1.4548e-02,  4.8718e-01,  2.9688e-01,\n",
      "         2.6364e-01,  3.4070e-01,  8.7643e-02, -1.3538e-01, -4.6405e-01,\n",
      "        -3.0432e-02,  6.0542e-01,  6.2298e-03,  5.2618e-01,  5.1252e-01,\n",
      "        -6.0583e-02,  3.2059e-01,  5.0795e-02, -3.7913e-01,  1.1509e-01,\n",
      "         4.5375e-02, -1.3565e-01,  2.1276e-01, -2.3581e-01, -8.1044e-02,\n",
      "        -1.0798e-01, -1.8571e-01, -1.0728e+00,  4.7328e-01, -1.6593e-01,\n",
      "        -1.2072e-01,  2.8705e-01,  9.9581e-02, -4.6817e-02, -3.8153e-01,\n",
      "        -1.1379e+00,  5.4662e-02, -3.9528e-01, -3.1390e-01,  2.3665e-01,\n",
      "         6.9778e-02, -2.0811e-01,  1.2890e-02, -4.6531e-01,  3.2057e-01,\n",
      "         9.4181e-02,  8.6495e-01,  1.6564e-01,  4.8025e-01,  3.7577e-01,\n",
      "        -1.4081e-01, -2.6507e-01, -7.3572e-02, -3.5227e-01, -1.8423e-02,\n",
      "        -1.6227e-01, -6.8800e-01, -3.7778e-01,  6.5797e-01,  5.6698e-01,\n",
      "        -1.6589e-01, -1.2800e-01, -3.0761e-01,  1.0017e-01,  1.1999e-01,\n",
      "         9.2364e-01, -5.6412e-01, -3.4964e-01,  8.6411e-01, -1.1267e-01,\n",
      "        -3.5904e-01,  2.2051e-01, -7.9063e-02, -3.5526e-01, -1.2467e-01,\n",
      "         1.5900e-01, -6.2419e-02, -2.2440e-01,  6.3188e-01,  7.1091e-01,\n",
      "        -2.3875e-01,  2.7848e-01, -2.1440e-01,  2.9097e-01, -3.0688e-01,\n",
      "        -8.6708e-01, -1.7800e-01, -4.2084e-01,  4.2230e-01,  3.0163e-01,\n",
      "         7.1566e-02,  3.7293e-01, -8.7315e-01, -4.0848e-01, -2.7700e-01,\n",
      "         7.3634e-02, -6.3365e-01,  1.7503e-01, -3.5598e-01, -9.9845e-02,\n",
      "         1.9582e-01, -3.4308e-01, -6.8154e-01,  5.8581e-02,  6.3393e-01,\n",
      "        -8.1310e-02, -2.2321e-01, -7.1545e-01, -4.5075e-01, -4.0600e-01,\n",
      "        -6.8357e-01,  2.9135e+00, -4.7896e-01, -1.4925e-01,  3.5409e-01,\n",
      "        -7.7365e-01, -3.3527e-01,  4.5086e-01,  1.4008e-01,  5.1023e-01,\n",
      "        -1.2088e-01, -2.3542e-01,  5.5585e-02,  3.6120e-01,  1.0468e-01,\n",
      "        -1.6335e-01, -3.4525e-01, -1.7934e-01,  9.2136e-02,  4.1637e-01,\n",
      "        -4.0313e-01, -5.4152e-01,  1.7409e-01,  1.1714e-02,  2.3976e-01,\n",
      "         1.7344e-01,  4.4706e-01, -6.2140e-01, -1.0343e-01,  3.3437e-01,\n",
      "         7.8736e-01, -9.1567e-02,  4.8067e-01,  6.5738e-01,  3.6144e-01,\n",
      "         4.8915e-02,  2.1303e-01,  4.0059e-01,  4.9710e-01,  5.0200e-01,\n",
      "         2.6840e-01,  5.0674e-01,  2.3797e-01, -1.2170e-01, -4.7272e-01,\n",
      "        -3.6212e-01,  2.8228e-01, -7.3108e-01, -6.0687e-01,  2.1979e-01,\n",
      "        -1.0977e-01, -5.7985e-01, -7.9890e-01, -5.1609e-01,  2.9611e-01,\n",
      "         8.7503e-02,  2.4944e-01, -2.1890e-02,  2.1669e-01,  5.2178e-01,\n",
      "        -4.2051e-01,  3.6763e-01,  7.9214e-01,  1.3569e-01, -1.5177e-01,\n",
      "        -3.5250e-01,  2.5388e-01, -6.2935e-01,  1.3537e-01,  7.6654e-01,\n",
      "        -5.3259e-01,  8.1554e-01,  5.6624e-02,  1.6385e-01,  3.0743e-01,\n",
      "        -5.7660e-01,  1.5037e-01,  1.5071e-01,  7.2490e-01,  3.9679e-01,\n",
      "        -8.0009e-01,  8.8988e-01, -6.4599e-01, -1.1653e-01,  5.3490e-01,\n",
      "        -4.2875e-02, -6.3929e-01,  2.5601e-01,  1.9401e-01,  7.6371e-02,\n",
      "        -6.6514e-01, -3.0451e-01, -6.4938e-02, -1.4087e-01,  1.2739e-01,\n",
      "        -1.3538e-01,  2.3661e-01,  9.4390e-01,  5.7977e-01,  1.4938e-02,\n",
      "        -2.0471e-02,  3.2937e-01,  2.9697e-01,  2.9512e-01,  1.8669e-01,\n",
      "         1.5200e-01,  1.6643e-01,  2.6421e-01, -4.1812e-01,  8.4733e-01,\n",
      "        -2.7933e-01,  6.9710e-02, -2.7250e-01, -8.2575e-01,  1.3175e-01,\n",
      "         1.0234e-01,  7.5936e-02,  7.1978e-01, -4.0852e-01,  1.1842e-01,\n",
      "         2.3656e-01,  5.0329e-01, -3.8426e-01, -2.4022e-01, -5.0387e-03,\n",
      "         1.2979e+00, -2.8820e-01,  1.6939e-01, -7.7244e-03,  8.3645e-02,\n",
      "         1.2011e-01,  8.5828e-02, -6.2860e-01,  1.2834e-01, -5.6821e-01,\n",
      "         2.0169e-01,  2.3180e-01,  7.7567e-01,  3.2506e-01,  6.3621e-02,\n",
      "         1.9635e-01, -4.1105e-01, -1.3785e-01, -1.2672e+01, -6.0461e-01,\n",
      "         5.4662e-01, -1.6453e-01, -4.1747e-01, -1.1156e-01,  2.5894e-01,\n",
      "         2.2227e-02, -3.5365e-01, -1.8938e-01,  1.1660e-02,  7.1135e-01,\n",
      "         9.1709e-01,  9.9433e-02, -8.2206e-01, -1.4940e-01,  6.9083e-02,\n",
      "         7.9197e-01,  2.0849e-01,  1.8751e-01, -5.0361e-01,  1.3976e-01,\n",
      "        -2.1762e-01, -5.2727e-01,  7.2799e-01,  2.5814e-01,  1.3059e-01,\n",
      "        -3.5964e-01, -3.7727e-01,  3.6870e-01,  5.5320e-01, -4.5154e-01,\n",
      "        -2.4548e-01, -5.4487e-01,  2.2874e-03, -1.3633e-01, -3.9987e-01,\n",
      "        -9.6901e-02,  2.4240e-01,  7.0434e-01, -1.2333e-01, -8.9731e-02,\n",
      "         2.2859e-01,  1.6612e-01, -1.8512e-01,  5.6885e-01,  2.0100e-01,\n",
      "         3.6616e-01,  1.5317e-01,  6.9105e-01,  1.9583e-02,  3.7611e-01,\n",
      "         1.0312e+00, -2.1457e-01, -3.9483e-01, -1.4665e-01,  6.2577e-01,\n",
      "        -1.8515e-01, -3.1915e-01, -6.2350e-01,  7.8260e-02, -2.1850e-01,\n",
      "         9.5003e-01,  2.0913e-01, -4.2788e-02, -7.6636e-01, -5.6977e-01,\n",
      "        -5.2268e-01, -1.2830e-01, -8.4175e-02,  4.1396e-01, -9.0383e-01,\n",
      "        -1.2172e-01, -3.2099e-01, -5.3960e-01, -1.7603e-01, -8.7361e-01,\n",
      "        -4.2969e-01, -7.5813e-01,  4.2659e-01,  3.9066e-01, -1.3467e+00,\n",
      "         1.7811e-01,  2.9514e-02,  3.8213e-01, -1.8032e-01, -5.3158e-01,\n",
      "        -4.9084e-01, -6.5023e-01,  6.8821e-02,  2.7723e-01, -2.2008e-01,\n",
      "        -1.7570e-01,  7.8770e-02,  2.6018e-01, -1.1979e-01,  5.0020e-01,\n",
      "         5.5483e-01,  7.3787e-02,  5.2920e-02,  3.7347e-01, -9.2976e-02,\n",
      "         5.1423e-02, -2.0620e-01,  3.9870e-01,  1.0556e-02, -3.0127e-01,\n",
      "         8.0947e-01, -8.8483e-02, -2.6117e-01,  2.0419e-01,  1.5946e-01,\n",
      "         7.7608e-01,  4.0682e-02, -2.3621e-01, -6.3481e-01,  1.7103e-01,\n",
      "        -4.3413e-01,  4.0318e-01,  5.0623e-01, -1.3230e-01,  6.7810e-01,\n",
      "        -3.3445e-01,  1.4430e-01, -3.3927e-01, -3.1590e-01, -3.3338e-01,\n",
      "         2.1299e-01,  3.3015e-02,  3.1188e-01, -3.4024e-01, -1.4752e-01,\n",
      "        -4.1744e-01,  6.6795e-02, -1.0811e-01,  3.1713e-02, -2.9733e-01,\n",
      "        -3.8682e-01, -2.0771e-02,  1.7287e-01,  2.7052e-02, -4.8108e-01,\n",
      "         1.9969e-01,  2.0850e-02, -8.6351e-02,  2.4049e-02, -7.2054e-01,\n",
      "        -2.4475e-01, -1.7721e-01,  4.2834e-01,  2.3319e-02, -2.9296e-01,\n",
      "         5.6954e-01,  6.4165e-01,  7.1479e-01,  1.8394e-01,  7.7559e-03,\n",
      "        -3.5321e-01,  1.5476e-01, -5.5245e-01, -2.1073e-01, -2.9440e-02,\n",
      "        -3.6585e-01, -5.1121e-02, -4.5107e-01,  1.5458e-01, -6.7454e-01,\n",
      "         1.2202e-01, -2.2382e-01,  6.6349e-01, -6.2558e-01,  6.4152e-01,\n",
      "        -1.4675e-01,  1.9345e-01,  2.4783e-01,  3.3400e-01,  3.7997e-01,\n",
      "         8.9359e-01,  1.4917e-01, -6.5576e-01,  8.7905e-02, -9.6151e-02,\n",
      "         1.2556e-01,  4.8867e-01, -1.0314e-01,  2.7125e-01,  2.7728e-01,\n",
      "        -2.5051e-01, -1.8205e-01,  1.5419e-01, -2.4971e-01,  6.2710e-01,\n",
      "         2.2956e-01,  4.2598e-01,  4.4557e-01, -7.8941e-01, -1.7454e-01,\n",
      "        -2.7174e-02,  6.0686e-01,  2.9939e-01, -2.4632e-01, -7.5258e-01,\n",
      "        -3.3717e-01, -2.1755e-01, -5.8688e-01, -5.5714e-01, -4.0085e-01,\n",
      "        -6.6194e-01,  3.5552e-02, -2.7823e-01])\n"
     ]
    }
   ],
   "source": [
    "# Embedding del token [CLS] que representa la frase completa\n",
    "cls_embedding = embeddings[0, 0, :]  # El primer [0] es la primera secuencia, el segundo [0] es el token [CLS]\n",
    "\n",
    "# Mostrar el tama√±o del embedding del token [CLS]\n",
    "print(\"Tama√±o del embedding del token [CLS]:\", cls_embedding.shape)\n",
    "print(\"Embedding del token [CLS]:\", cls_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embeddings por palabra\n",
    "Tambi√©n podemos ver los embeddings para cada token de la frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS] -> Embedding: tensor([ 0.0820, -0.1386,  0.2357, -0.1808,  0.4499])... (truncado)\n",
      "Token: La -> Embedding: tensor([ 0.0681,  0.6668, -0.7685, -0.2641,  0.3665])... (truncado)\n",
      "Token: inteligencia -> Embedding: tensor([-0.1536, -0.4845, -0.4437,  0.1451, -0.2135])... (truncado)\n",
      "Token: artificial -> Embedding: tensor([-0.5083, -0.5315,  0.1694, -0.4105,  0.2094])... (truncado)\n",
      "Token: est√° -> Embedding: tensor([-0.1063,  0.2039,  0.0624, -0.3050,  0.4447])... (truncado)\n",
      "Token: transforma -> Embedding: tensor([-0.4565,  0.3401, -0.7017,  0.3512, -0.0459])... (truncado)\n",
      "Token: ##n -> Embedding: tensor([-0.5090,  0.5637, -0.9005,  0.3523,  0.3229])... (truncado)\n",
      "Token: ##do -> Embedding: tensor([-0.4623,  0.4462, -1.0330,  0.2840,  0.3256])... (truncado)\n",
      "Token: el -> Embedding: tensor([-0.8480,  0.8487, -0.4200,  0.7275, -0.1490])... (truncado)\n",
      "Token: mundo -> Embedding: tensor([-0.5589,  0.2899, -0.1544,  0.3578, -0.2566])... (truncado)\n",
      "Token: . -> Embedding: tensor([-0.3829,  0.3332,  0.1027, -0.0988,  0.2425])... (truncado)\n",
      "Token: [SEP] -> Embedding: tensor([-1.4672, -0.5293,  0.0109,  1.0246, -0.4454])... (truncado)\n"
     ]
    }
   ],
   "source": [
    "# Embeddings para cada token en la frase\n",
    "for token, embedding in zip(tokens, embeddings[0]):\n",
    "    print(f\"Token: {token} -> Embedding: {embedding[:5]}... (truncado)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
